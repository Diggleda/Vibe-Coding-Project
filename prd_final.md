# Star Chart Systems — Final PRD (Phase 3)

## 1) Product Overview (Updated)

Manufacturing teams often know they have bottlenecks, micro-stops, and flow variability, but they lack an easy way to *see* what’s happening in real time and translate telemetry into clear actions. The problem is especially acute for small and mid-sized factories that can’t afford enterprise analytics stacks and end up relying on spreadsheets, gut feel, and sporadic time studies.

Star Chart Systems is a single-page dashboard prototype that visualizes a factory flow map with simulated high-frequency telemetry (units moving through stages, velocity changes, dwell time, congestion approximations) and adds a “Star Chart AI” chat assistant. Users can ask natural-language questions about the current flow, bottlenecks, and suggested interventions. The current prototype focuses on a compelling demo: live visualization + explainable metrics + AI recommendations, rather than full production integrations.

## 2) Core Features & Status

| Feature | Status | AI-dependent? |
|---|---:|---:|
| Live “Constellation Map” factory flow visualization (canvas) | Implemented | No |
| Simulated 12 Hz telemetry stream (units, positions, stages) | Implemented | No |
| Station status cards, deviations table, alerts feed | Implemented | No |
| Congestion model panel (Kingman VUT approximation) | Implemented | No |
| Filters (fastest/slowest/cohorts) | Implemented | No |
| Pod coverage overlay + toggle (uniform radius) | Implemented | No |
| Star Chart AI chat UI (message log + input) | Implemented | Yes |
| AI telemetry context payload (units + velocities + dwell + congestion) | Implemented | Yes |
| Hosted AI proxy for GitHub Pages (Cloudflare Worker) | Implemented | Yes (enables AI) |
| Authenticated user accounts / roles | Future | No |
| Real factory data ingestion (UWB/pod feeds) | Future | No |
| Persistent chat history per user/session | Future | Yes |
| Evaluation harness (recommendation quality, safety checks) | Future | Yes |

## 3) AI Specification (Final)

### What the AI does
The AI acts as an operations-focused assistant that:
- Interprets the current telemetry snapshot (unit-level velocity, dwell, stage distribution, congestion estimates).
- Answers questions (“what’s the bottleneck?”, “why is Wq up?”, “what should I change?”).
- Produces concise, actionable recommendations aligned to the visible metrics (e.g., buffer sizing, balancing work content, targeting a specific stage).

### Inputs and outputs
**Inputs**
- User text prompt from the chat UI.
- A telemetry snapshot object including:
  - `units[]` with `id`, `stage`, `status`, `progress`, `x`, `y`, `velocityMps`, `dwellSeconds`
  - stage counts/averages, velocity/dwell summaries
  - congestion estimates (`rho`, `wqSeconds`, `totalSeconds`) by stage
  - pod geometry and UI state (pod coverage toggle)

**Outputs**
- A single assistant message with explanation + suggested actions in plain language (no charts generated by the model).

### Where it appears in the user flow
The AI appears directly on the main dashboard page as “Star Chart AI” above the factory map. The user can ask questions at any time while the telemetry updates; the AI response is appended to the chat log.

### Model / tool used
- OpenAI text model configured via environment variable:
  - Local dev: `CCD_AI_OPENAI_MODEL` in `.env`
  - Worker: `OPENAI_MODEL` variable in Cloudflare
- Current default target model: `gpt-4.1`
- API: OpenAI Responses API (`POST /v1/responses`) via the proxy

### Constraints / guardrails
Implemented guardrails in the proxy + client:
- **No client-side secret exposure**: API key lives only in the server/worker environment.
- **Origin allow-list**: `ALLOWED_ORIGINS` can restrict which website origins may call the proxy (CORS).
- **Bounded inputs**: chat inputs are truncated; history is limited to a recent window; telemetry is structured and generated by the app.
- **Bounded outputs**: output token limit is set (short answers preferred); temperature is conservative for stability.

Not yet implemented (known gap):
- No formal rate limiting / abuse prevention beyond basic payload constraints.
- No content moderation or domain-specific safety classifier; responses are advisory only.

## 4) Technical Architecture (Reality Check)

### Front-end
- Single static page: `index.html`
- Styling: Tailwind via CDN
- Visualization: HTML `<canvas>` for the map + overlays
- Data: simulated telemetry generated in-browser at 12 Hz

### AI call path (GitHub Pages-compatible)
The browser never calls OpenAI directly. It calls a proxy:

```
Browser (GitHub Pages)
  └─ POST https://<worker-domain>/api/chat
       └─ Worker calls OpenAI Responses API
            └─ returns assistant text to browser
```

### External services
- OpenAI API (LLM inference)
- Cloudflare Workers (serverless proxy)
- GitHub Pages (static hosting for the UI)

## 5) Prompting & Iteration Summary

Vibe coding was used to move fast from “cool dashboard” to “working AI feature on GitHub Pages”:

Key prompts that drove progress:
1. “Add a simple generative chatbot feature. Make it as quickly as possible.”
   - Result: added chat UI wiring and an initial local proxy approach.
2. “Use open AI and keys.”
   - Result: moved the API key off the browser and into a server-side call path; added `.env` support.
3. “This will be running on GitHub pages. The AI needs to be ready to go seamlessly.”
   - Result: recognized static hosting constraints; introduced a serverless proxy (Worker) and a configurable proxy base URL.
4. “Good. Also give the AI all the context. Including velocities.”
   - Result: expanded telemetry snapshot to include unit-level velocity/dwell/position plus summaries and congestion-by-stage.

How prompts evolved:
- Started with “make a chatbot,” then quickly shifted to “make it deployable on static hosting,” which required architectural changes (proxy + CORS).
- Prompts became more specific about UX (“move chat above map”, “simplify label”) and data fidelity (“include velocities”).

What I learned about prompt design:
- The fastest progress came from prompts that included deployment constraints early (e.g., “GitHub Pages”).
- Asking for “all context” needed follow-up specificity (which signals matter: velocity, dwell, congestion, stage distributions).
- UI/UX prompts worked best when they named exact text to remove and the desired replacement.

## 6) UX & Limitations

### Intended user journey
1. Open the dashboard and observe the live map + metrics panels.
2. Spot visible risk signals (high utilization, high dwell, deviation spikes).
3. Ask Star Chart AI: “What’s the constraint right now and what should I do first?”
4. Apply recommended actions conceptually (rebalance work, increase buffer, address micro-stops) and watch how metrics respond in the simulation.

### Known limitations (“janky bits”)
- Telemetry is simulated, not real sensor data.
- AI responses are based on the current snapshot and may be generic; there’s no evaluation harness proving accuracy.
- No authentication; anyone with access to the page can use the AI endpoint unless restricted by `ALLOWED_ORIGINS` and Cloudflare settings.
- Tailwind via CDN is fine for a prototype but not optimized for production.

### Ethical / trust limitations
- Do not rely on the AI for safety-critical decisions (e.g., worker safety, compliance, shutdown criteria).
- The AI may provide plausible but incorrect operational advice; recommendations should be validated by an engineer and real measurements.
- The AI has no guaranteed understanding of your actual factory context unless you integrate real data and validation.

## 7) Future Roadmap

- Replace simulated telemetry with real ingestion (UWB tags/pods → pipeline → dashboard).
- Add AI evaluation: test prompts, expected answers, scoring + regression checks for recommendation quality.
- Add rate limiting + abuse controls (per-IP quotas, captcha, request logging/alerts) on the proxy.
- Add user sessions and “decision log”: save questions, AI recommendations, and outcomes for iteration.
- Add safety features: explicit uncertainty, “ask for confirmation” patterns, and domain constraints to reduce overconfident advice.

