## Implementation & Ethics Memo — Star Chart Systems

This memo describes how I actually built my Star Chart Systems prototype, why the AI feature ended up looking the way it does, and what trade-offs I made around safety, privacy, and integrity. The short version is that I used generative AI as a rapid “pair programmer” to ship a working demo in VS Code, and then I had to confront the reality that a demo that works on a laptop is not the same as a demo that works on GitHub Pages. That constraint shaped both the architecture and the ethical posture of the project.

When I started, I had a single-page dashboard concept: a visual “constellation map” of factory flow, updated frequently enough to feel like real telemetry. The early value proposition was clarity—turn motion and station metrics into something you can understand quickly. My initial focus was therefore conventional UI and simulation logic: a canvas map, stage markers, live updating metrics, and a stream of synthetic sensor data. The AI feature was intended to be an “interpretation layer” on top of that: instead of making users read a dozen metrics, they could ask one question and get a succinct recommendation.

### 1. How I Actually Used AI While Building

I used vibe coding tools primarily to accelerate implementation work that would have been slow or error-prone by hand: wiring UI interactions, adding server endpoints, diagnosing CORS issues, and reshaping code so it worked under deployment constraints. In practice, I treated the model like an assistant that could produce a first draft quickly, and then I used my own judgment to keep changes minimal, coherent, and secure. My primary coding environment was VS Code, and my primary model/tooling during development was `gpt-5.1-codex-max`.

The biggest chunk of AI assistance went into implementing the chatbot feature in a way that didn’t leak secrets. The naive path—calling OpenAI from the browser—would have been the fastest to code, but it is not acceptable for a public deployment because the API key would be exposed to anyone who opens DevTools. I used my own OpenAI API keys and routed requests through a Cloudflare Worker proxy for the backend. I chose a Worker primarily due to familiarity and speed: it is a lightweight, serverless way to keep secrets off the client while still working seamlessly with GitHub Pages.

AI was also helpful for debugging integration problems that only show up when you run things end-to-end: method errors, preflight requests, mixed content, and “wrong server answering the route.” A good example was the recurring `405` and `404` errors from `POST /api/chat`. The model helped reason about how browsers behave (preflight `OPTIONS`), and it suggested changes to make the endpoint tolerant to those requests. It also helped me recognize that my page was sometimes “connecting” to a different local dev server that happened to respond to `/api/health`, which created confusing partial success. I fixed this by adding an explicit “app signature” to health responses and validating it on the client.

While AI could generate code quickly, human editing mattered most in three areas. First, I had to enforce security boundaries: the assistant will happily paste keys into config files or client code if you ask. I had to stop and choose the safer pattern (secrets set as environment variables in the Worker, never committed). Second, I had to align the solution with the actual hosting environment. GitHub Pages is static, so any approach that assumes a server process is running “behind the scenes” is wrong. Third, I had to keep the feature scoped: the AI can easily balloon into a “do everything” assistant, but a capstone prototype needs a coherent, demonstrable interaction.

### 2. Why the AI Feature in Your Product Looks the Way It Does

The AI feature is deliberately simple: a single chat box embedded in the dashboard that answers questions about the current telemetry and recommends next actions. I chose this because it maps cleanly onto the core value proposition of the product. The prototype’s non-AI components generate a lot of signals—velocity, dwell time, stage congestion estimates, and status states. The AI’s job is to interpret those signals and communicate them in the language an operator or engineer would use.

I placed the AI component at the top of the page intentionally to highlight the convenience the product is aiming for: fast access to “what matters right now” without requiring users to hunt through panels. In addition to ad hoc Q&A, this interaction is a foundation for future automated notifications and proactive analysis (for example, the system generating comprehensive plans to reduce bottlenecks based on recurring patterns).

I chose this feature rather than something more ambitious (automatic optimization, scheduling, or anomaly detection) for a practical reason: those features require ground truth, careful evaluation, and a larger data pipeline. A chat assistant can be valuable even with synthetic data because the interaction is the point: it demonstrates how a user could interrogate a live system without needing to learn every metric. It also makes the demo feel coherent: the map shows movement; the metric cards summarize; the AI explains “what it means and what to do.”

I simplified aggressively to keep the system demonstrable. The assistant does not issue commands, modify system state, or trigger automation. It only returns text. It does not persist conversations in a database. It does not “learn” from users. The AI sees a telemetry snapshot (including per-unit velocities and dwell times) plus a short rolling window of the recent chat history, and then it responds. This is enough to show the intended future interaction without adding fragile infrastructure.

One place where the AI feature still doesn’t fully connect is that the telemetry is synthetic. That is intentional for a prototype, but it means the assistant’s advice should be treated as illustrative. The longer-term value would come from integrating real telemetry and then validating that recommendations correlate with improvements in throughput or cycle time. In this course version, the AI’s output is best understood as a “narrator” over a simulated system.

### 3. Risks, Trade-offs, and Integrity

The biggest security risk in AI prototypes is secret leakage. In a browser-hosted app, you cannot safely embed an OpenAI API key. My explicit choice was therefore to route all OpenAI calls through a server-side component. Because the product is hosted on GitHub Pages (static), the server-side component became a serverless Worker. This is not just an engineering choice; it is also an ethics and integrity choice. If I had shipped the key in the browser, I would be asking users to trust a design that is fundamentally insecure, and I would be enabling misuse or unexpected charges.

Privacy is the next risk. Even though the telemetry is synthetic in this prototype, the design pattern matters. In a real factory deployment, telemetry could be sensitive: it can reveal production volumes, station efficiency, staffing patterns, and potentially employee performance. My design keeps the user-facing payload constrained to operational metrics, but it still sends structured telemetry to the model. The ethical implication is that a production version would need explicit data minimization (send only what is necessary), clear retention policies, and stronger access control. It would also require careful consideration of whether employee-level tracking is appropriate, and how to avoid turning an analytics tool into a surveillance tool.

Bias and fairness show up differently in this project than they do in consumer-facing recommendation engines. The risk is not demographic bias in the classic sense, but operational bias: the model may over-recommend interventions that increase throughput at the expense of safety, ergonomics, or worker fatigue. This is one reason I constrained the AI to advisory text only and framed the assistant as an “operations engineer,” not an authority. The UI and the memo-level messaging both imply that recommendations should be validated. In future iterations, I would add explicit disclaimers in the interface itself and include “safety-first” constraints in the system instructions.

Over-reliance and trust is another major issue. Language models can sound confident even when they are wrong, and in an operational setting that can cause harm. Ethically, this prototype does not introduce a large set of new concerns beyond what a typical dashboard would, but it does create a responsibility to ensure the *presentation of data is grounded in truth* and that AI guidance is framed appropriately. In particular, the system prompt should stay within the project’s guidelines for AI automated suggestions and notifications about production bottlenecks. I addressed this partially through scope limitation: the AI has no direct control over equipment, and it cannot trigger actions automatically. I also kept generation parameters conservative (lower creativity) and bounded the output length. However, these are not complete solutions. A more serious system would include calibrated uncertainty, citations to which metrics drove the recommendation, and an evaluation workflow that flags low-confidence advice.

Academic integrity matters too: I used AI tools heavily, but I treated them as assistance rather than as a substitute for understanding. The work that matters for integrity is the judgment: choosing a secure architecture, deciding what to include in telemetry context, debugging why requests failed on Pages, and editing the UI so it communicates clearly. I also recognize that some work products (like chunks of glue code) were generated quickly with assistance. In a professional context I would document that in commit messages or a “credits” section; in this class context I am being explicit in this memo about where AI was involved and where I made final decisions.

Finally, I made explicit choices to limit what the AI does. The assistant does not store user content. It does not request personal information. It does not produce decisions that are framed as guarantees. It is a conversational layer on top of metrics, not an automation engine. Those constraints reduced both implementation complexity and ethical risk.

### 4. What I Learned About Building with GenAI

The biggest surprise was how quickly the problem shifted from “can I generate code?” to “can I deploy it correctly and safely?” The hardest part wasn’t getting a chat box to display text. It was dealing with the unglamorous integration details: CORS rules, HTTPS restrictions on GitHub Pages, the fact that “localhost” approaches don’t translate to public hosting, and the need to keep secrets out of repositories. AI assistance helped, but it still required me to understand the environment and make disciplined architectural choices.

If I had to teach another founder one thing about using GenAI tools well, it would be this: be specific about constraints early. When I added “this is going on GitHub Pages” to the conversation, the solution space changed. Prompts that include deployment target, security requirements, and user expectations lead to much better outcomes than prompts that only describe features. Another lesson is to treat AI-generated code as a draft. The best workflow is: generate, review, simplify, and then test in the real environment where users will run it.

This project also changed how I think about AI in future ventures. The most valuable AI features are not necessarily the most complex. A small, well-integrated assistant can add a lot of perceived value if it sits at the right point in the user journey. But shipping that feature responsibly requires treating privacy and security as first-class product requirements, not afterthoughts. In future work, I would invest earlier in evaluation and guardrails: define what “good advice” means, create test scenarios, log outcomes, and build a feedback loop that improves reliability over time.

Overall, building Star Chart Systems reinforced a balanced view of GenAI: it is an excellent accelerator for prototyping and integration, but it also increases the importance of human judgment in architecture, security, and user trust.
